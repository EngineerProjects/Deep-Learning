{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/futuremojo/nlp-demystified/blob/main/notebooks/nlpdemystified_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DM8kLxUEVc3Z"
   },
   "source": [
    "# Natural Language Processing Demystified | Preprocessing\n",
    "https://nlpdemystified.org<br>\n",
    "https://github.com/futuremojo/nlp-demystified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btimL_w92Q3P"
   },
   "source": [
    "### spaCy upgrade and package installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7Ll-fUK2VZs"
   },
   "source": [
    "At the time this notebook was created, spaCy had newer releases but Colab was still using version 2.x by default. So the first step is to upgrade spaCy.\n",
    "<br><br>\n",
    "**IMPORTANT**<br>\n",
    "If you're running this in the cloud rather than using a local Jupyter server on your machine, then the notebook will **timeout** after a period of inactivity. If that happens and you don't reconnect in time, you will need to upgrade spaCy again and reinstall the requisite statistical packages.\n",
    "<br><br>\n",
    "Refer to this link on how to run Colab notebooks locally on your machine to avoid this issue:<br>\n",
    "https://research.google.com/colaboratory/local-runtimes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cve1-G7j2VTN"
   },
   "outputs": [],
   "source": [
    "# !pip install -U spacy==3.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "z-FDdbc62VHd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================== Info about spaCy ==============================\u001b[0m\n",
      "\n",
      "spaCy version    3.8.5                         \n",
      "Location         /home/amiche/anaconda3/envs/nlp/lib/python3.12/site-packages/spacy\n",
      "Platform         Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "Python version   3.12.9                        \n",
      "Pipelines                                      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8vW9svTE289D"
   },
   "outputs": [],
   "source": [
    " import spacy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfJKSJEU2U_s"
   },
   "source": [
    "After importing spaCy, the next thing we need to do is load a suitable statistical model for our project. spaCy offers a variety of models for different languages. These models help with tokenization, part-of-speech tagging, named entity recognition, and more.\n",
    "\n",
    "Here, we're loading the **en_core_web_sm** model which is the smallest English model spaCy offers and is a good starting point for NLP tasks.<br>\n",
    "https://spacy.io/models/en#en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6v6TGQff2iu6"
   },
   "source": [
    "Since we upgraded spaCy, we'll need to download the statistical model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4uOyHDNb2i5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mWDrpxDk2_r2"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7YCbWtG3LJO"
   },
   "source": [
    "**en_core_web_sm** is trained on OntoNotes 5 which is an annotated corpus comprising news, blogs, transcripts, etc. Put simply, this means a bunch of documents were labelled with information such as how each sentence should be parsed, whether a particular word is a noun or adjective or other part-of-speech, whether a word is a special entity like a person or a real-world organization, and other language-related labels. A statistical model was then generated from these labelled documents.<br>\n",
    "https://catalog.ldc.upenn.edu/LDC2013T19\n",
    "<br><br>\n",
    "You can learn more about the available spaCy models at these links:<br>\n",
    "https://spacy.io/models<br>\n",
    "https://spacy.io/usage/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvF_udvi3OTO"
   },
   "source": [
    "After loading the model, the _nlp_ variable now references a **Language** class instance which contains language-specific rules for various tasks (e.g. tokenization) and a processing pipeline.<br>\n",
    "https://spacy.io/api/language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DAYGtQpT3UNN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unmnGRu8D-wa"
   },
   "source": [
    "# Tokenization\n",
    "\n",
    "Course module for this demo:\n",
    "https://www.nlpdemystified.org/course/tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLUcGm3IbQki"
   },
   "source": [
    "### Tokenization with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13twUCp2i_p8"
   },
   "source": [
    "We pass whatever text we want to process to _nlp_, which returns a **Doc** container object containing the tokenized text and a number of annotations for each token. These annotations are discussed in follow-up videos. You can learn more about the **Doc** object here:<br>\n",
    "https://spacy.io/api/doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BIoEJZ-IkHQ4"
   },
   "outputs": [],
   "source": [
    "# Sample sentence.\n",
    "s = \"He didn't want to pay $20 for this book.\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"He didn't want to pay $20 for this book.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMWZK3ZSk9-f"
   },
   "source": [
    "We can iterate over this **Doc** object and view the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8SzqhZuulAe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'did', \"n't\", 'want', 'to', 'pay', '$', '20', 'for', 'this', 'book', '.']\n"
     ]
    }
   ],
   "source": [
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ai1obkB93GdD"
   },
   "source": [
    "Note how\n",
    "- \"didn't\" is separated into \"did\"  and \"n't\".\n",
    "- the currency symbol and amount are separated.\n",
    "- the period at the end of the sentence is its own token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWH49gIh3hqN"
   },
   "source": [
    "The **Doc** object can be indexed and sliced like a regular list. The **Doc** object contains **Token** and **Span** objects, which offer different views into the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MwLrxRsE3oKI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He\n"
     ]
    }
   ],
   "source": [
    "# We can view an individual token by indexing into the Doc object.\n",
    "print(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bGapNHYQFYVa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# A Doc object is a container of other objects, namely Token and Span objects.\n",
    "print(type(doc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EtL2IgIAGOd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He didn't\n",
      "<class 'spacy.tokens.span.Span'>\n"
     ]
    }
   ],
   "source": [
    "# Slicing a Doc object returns a Span object.\n",
    "print(doc[0:3])\n",
    "print(type(doc[0:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xybH4jjYGo73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 0), ('did', 1), (\"n't\", 2), ('want', 3), ('to', 4), ('pay', 5), ('$', 6), ('20', 7), ('for', 8), ('this', 9), ('book', 10), ('.', 11)]\n"
     ]
    }
   ],
   "source": [
    "# Access a token's index in a sentence.\n",
    "print([(t.text, t.i) for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TqE980F4Vrt"
   },
   "source": [
    "Spacy's tokenization is _non-destructive_, which means the original input can be reconstructed from the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OjXb8mR_DK-1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He didn't want to pay $20 for this book.\n"
     ]
    }
   ],
   "source": [
    "# You can view the original input like so:\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73vuSX7MDK79"
   },
   "source": [
    "You can learn more about the **Token** and **Span** objects here:<br>\n",
    "https://spacy.io/api/token<br>\n",
    "https://spacy.io/api/span\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        He | PRON       | nsubj      | head: want\n",
      "       did | AUX        | aux        | head: want\n",
      "       n't | PART       | neg        | head: want\n",
      "      want | VERB       | ROOT       | head: want\n",
      "        to | PART       | aux        | head: pay\n",
      "       pay | VERB       | xcomp      | head: want\n",
      "         $ | SYM        | nmod       | head: 20\n",
      "        20 | NUM        | dobj       | head: pay\n",
      "       for | ADP        | prep       | head: pay\n",
      "      this | DET        | det        | head: book\n",
      "      book | NOUN       | pobj       | head: for\n",
      "         . | PUNCT      | punct      | head: want\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"He didn't want to pay $20 for this book.\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:>10} | {token.pos_:<10} | {token.dep_:<10} | head: {token.head.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lume_1UP6ySQ"
   },
   "source": [
    "We can also tokenize multiple sentences and access each sentence individually using the **Doc** object's _sents_ property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "mPZ86x0hDK4m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences : [Either the well was very deep, or she fell very slowly, for she \n",
      "had plenty of time as she went down to look about her and to wonder what \n",
      "was going to happen next., First, she tried to look down and make out what \n",
      "she was coming to, but it was too dark to see anything; then she looked at \n",
      "the sides of the well, and noticed that they were filled with cupboards and \n",
      "book-shelves; here and there she saw maps and pictures hung upon pegs.]\n",
      "\n",
      "Number of sentence : 2\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\"Either the well was very deep, or she fell very slowly, for she \n",
    "had plenty of time as she went down to look about her and to wonder what \n",
    "was going to happen next. First, she tried to look down and make out what \n",
    "she was coming to, but it was too dark to see anything; then she looked at \n",
    "the sides of the well, and noticed that they were filled with cupboards and \n",
    "book-shelves; here and there she saw maps and pictures hung upon pegs.\"\"\"\n",
    "\n",
    "doc = nlp(s)\n",
    "\n",
    "# Look at individual sentences (there should be two 'Span' objects).\n",
    "sentences = [sent for sent in doc.sents]\n",
    "print(f\"Sentences : {sentences}\")\n",
    "print(f\"\\nNumber of sentence : {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvSfDUyK06Qg"
   },
   "source": [
    "### Tokenization Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyywcBrCHzSk"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE:\n",
    "# 1) Tokenize the following text\n",
    "# 2) Iterate through the tokens to check whether there's a currency symbol.\n",
    "# 3) If there is, and the currency label is followed by a number, print\n",
    "#    both the symbol and the number.\n",
    "# \n",
    "# Look through https://spacy.io/api/token#attributes on how to check whether\n",
    "# a token is a currency symbol or a number.\n",
    "#\n",
    "# Expected output: \"$20\".\n",
    "s = \"He didn't want to pay $20 for this book.\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens : [('He', 0), ('did', 1), (\"n't\", 2), ('want', 3), ('to', 4), ('pay', 5), ('$', 6), ('20', 7), ('for', 8), ('this', 9), ('book', 10), ('.', 11)]\n",
      "$20\n"
     ]
    }
   ],
   "source": [
    "s = \"He didn't want to pay $20 for this book.\"\n",
    "doc = nlp(s)\n",
    "\n",
    "print(f\"tokens : {[(token.text, token.i) for token in doc]}\")\n",
    "print(doc[6:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skajI-OZDK0t"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Learn how the spaCy tokenizer works and how to customize it:\n",
    "# https://spacy.io/usage/linguistic-features#tokenization\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple plit : ['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$1', 'billion']\n",
      "\n",
      "Tokenization : ['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "\n",
    "# simple split\n",
    "split = [elt for elt in text.split(\" \")]\n",
    "print(f\"Simple plit : {split}\")\n",
    "# tokenization\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "print(f\"\\nTokenization : {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal Python Tokenize (spaCy-style logic)\n",
    "def custom_tokenizer(text, special_cases, prefix_search, suffix_search, infix_finditer, token_match, url_match):\n",
    "    tokens = []\n",
    "\n",
    "    # Step 1: Split text by whitespace to get initial word candidates\n",
    "    for substring in text.split():\n",
    "        while substring:\n",
    "            # Step 2: Check for full special case (e.g., \"U.K.\", \"don't\")\n",
    "            if substring in special_cases:\n",
    "                tokens.extend(special_cases[substring])\n",
    "                break\n",
    "\n",
    "            # Step 3: Repeatedly handle prefixes and suffixes\n",
    "            while prefix_search(substring) or suffix_search(substring):\n",
    "                # Step 3a: Check token match (e.g., emoji, number, email, etc.)\n",
    "                if token_match(substring):\n",
    "                    tokens.append(substring)\n",
    "                    substring = \"\"\n",
    "                    break\n",
    "\n",
    "                # Step 3b: Re-check for special case\n",
    "                if substring in special_cases:\n",
    "                    tokens.extend(special_cases[substring])\n",
    "                    substring = \"\"\n",
    "                    break\n",
    "\n",
    "                # Step 3c: Consume prefix\n",
    "                prefix = prefix_search(substring)\n",
    "                if prefix:\n",
    "                    split = prefix.end()\n",
    "                    tokens.append(substring[:split])\n",
    "                    substring = substring[split:]\n",
    "                    continue\n",
    "\n",
    "                # Step 3d: Consume suffix\n",
    "                suffix = suffix_search(substring)\n",
    "                if suffix:\n",
    "                    split = suffix.start()\n",
    "                    tokens.append(substring[split:])\n",
    "                    substring = substring[:split]\n",
    "                    continue\n",
    "\n",
    "            # Step 4: Token match again\n",
    "            if token_match(substring):\n",
    "                tokens.append(substring)\n",
    "                substring = \"\"\n",
    "\n",
    "            # Step 5: URL match\n",
    "            elif url_match(substring):\n",
    "                tokens.append(substring)\n",
    "                substring = \"\"\n",
    "\n",
    "            # Step 6: Special case (again)\n",
    "            elif substring in special_cases:\n",
    "                tokens.extend(special_cases[substring])\n",
    "                substring = \"\"\n",
    "\n",
    "            # Step 7: Handle infixes (e.g., hyphen, apostrophe inside word)\n",
    "            elif list(infix_finditer(substring)):\n",
    "                infixes = list(infix_finditer(substring))\n",
    "                offset = 0\n",
    "                for match in infixes:\n",
    "                    if offset != match.start():\n",
    "                        tokens.append(substring[offset:match.start()])\n",
    "                    tokens.append(substring[match.start():match.end()])\n",
    "                    offset = match.end()\n",
    "                if offset < len(substring):\n",
    "                    tokens.append(substring[offset:])\n",
    "                substring = \"\"\n",
    "\n",
    "            # Step 8: No special handling matched\n",
    "            else:\n",
    "                tokens.append(substring)\n",
    "                substring = \"\"\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def print_tokenization_results(results: dict):\n",
    "    for i, (sentence, tokens) in enumerate(results.items(), 1):\n",
    "        print(f\"\\n🟡 Example {i}:\")\n",
    "        print(f\"🔹 Input: {sentence}\")\n",
    "        print(f\"🔹 Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟡 Example 1:\n",
      "🔹 Input: Apple is looking at buying U.K. startup for $1 billion.\n",
      "🔹 Tokens: ['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', '.', 'billion']\n",
      "\n",
      "🟡 Example 2:\n",
      "🔹 Input: She said, 'I can't do this anymore!'\n",
      "🔹 Tokens: ['She', 'said,', \"'\", 'I', 'ca', \"n't\", 'do', 'this', 'anymore!', \"'\"]\n",
      "\n",
      "🟡 Example 3:\n",
      "🔹 Input: Please send an e-mail to support@example.com.\n",
      "🔹 Tokens: ['Please', 'send', 'an', 'e', '-', 'mail', 'to', '.', 'support@example.com']\n",
      "\n",
      "🟡 Example 4:\n",
      "🔹 Input: O'Neill is visiting the U.S. next week.\n",
      "🔹 Tokens: ['O', \"'\", 'Neill', 'is', 'visiting', 'the', '.', 'U.S', 'next', '.', 'week']\n",
      "\n",
      "🟡 Example 5:\n",
      "🔹 Input: He paid $100 for that (surprisingly expensive) book.\n",
      "🔹 Tokens: ['He', 'paid', '$', '100', 'for', 'that', '(', 'surprisingly', 'expensive)', '.', 'book']\n",
      "\n",
      "🟡 Example 6:\n",
      "🔹 Input: Visit http://example.com for more info.\n",
      "🔹 Tokens: ['Visit', 'http://example.com', 'for', 'more', '.', 'info']\n",
      "\n",
      "🟡 Example 7:\n",
      "🔹 Input: I don't think this will work.\n",
      "🔹 Tokens: ['I', 'do', \"n't\", 'think', 'this', 'will', '.', 'work']\n",
      "\n",
      "🟡 Example 8:\n",
      "🔹 Input: The price is $2.50!\n",
      "🔹 Tokens: ['The', 'price', 'is', '$', '!', '2.50']\n",
      "\n",
      "🟡 Example 9:\n",
      "🔹 Input: Welcome to A.I.-powered platforms.\n",
      "🔹 Tokens: ['Welcome', 'to', 'A.I.', '-', 'powered', '.', 'platforms']\n",
      "\n",
      "🟡 Example 10:\n",
      "🔹 Input: Don't worry – we've got you covered.\n",
      "🔹 Tokens: ['Don', \"'\", 't', 'worry', '–', 'we', \"'\", 've', 'got', 'you', '.', 'covered']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# ----------- Rule Definitions -----------\n",
    "PREFIXES = [r\"\\$\", r\"\\(\"]\n",
    "SUFFIXES = [r\"\\.\", r\"\\!\", r\"\\?\"]\n",
    "INFIXES = [r\"-\", r\"'\"]\n",
    "\n",
    "prefix_re = re.compile(r\"^(\" + \"|\".join(PREFIXES) + \")\")\n",
    "suffix_re = re.compile(r\"(\" + \"|\".join(SUFFIXES) + r\")$\")\n",
    "infix_re = re.compile(\"|\".join(INFIXES))\n",
    "\n",
    "prefix_search = lambda text: prefix_re.match(text)\n",
    "suffix_search = lambda text: suffix_re.search(text)\n",
    "infix_finditer = lambda text: infix_re.finditer(text)\n",
    "token_match = lambda text: None  # Simulated matcher\n",
    "url_match = lambda text: text.startswith(\"http\")\n",
    "\n",
    "SPECIAL_CASES = {\n",
    "    \"U.K.\": [\"U.K.\"],\n",
    "    \"don't\": [\"do\", \"n't\"],\n",
    "    \"can't\": [\"ca\", \"n't\"],\n",
    "    \"e-mail\": [\"e\", \"-\", \"mail\"],\n",
    "    \"O'Neill\": [\"O\", \"'\", \"Neill\"]\n",
    "}\n",
    "\n",
    "# ----------- Test Examples -----------\n",
    "examples = [\n",
    "    \"Apple is looking at buying U.K. startup for $1 billion.\",\n",
    "    \"She said, 'I can't do this anymore!'\",\n",
    "    \"Please send an e-mail to support@example.com.\",\n",
    "    \"O'Neill is visiting the U.S. next week.\",\n",
    "    \"He paid $100 for that (surprisingly expensive) book.\",\n",
    "    \"Visit http://example.com for more info.\",\n",
    "    \"I don't think this will work.\",\n",
    "    \"The price is $2.50!\",\n",
    "    \"Welcome to A.I.-powered platforms.\",\n",
    "    \"Don't worry – we've got you covered.\"\n",
    "]\n",
    "\n",
    "# Tokenize each example\n",
    "results = {text: custom_tokenizer(\n",
    "    text,\n",
    "    SPECIAL_CASES,\n",
    "    prefix_search,\n",
    "    suffix_search,\n",
    "    infix_finditer,\n",
    "    token_match,\n",
    "    url_match\n",
    ") for text in examples}\n",
    "\n",
    "print_tokenization_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not handle : ['gimme', 'that']\n",
      "Special case : ['gim', 'me', 'that', 'again']\n"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "import spacy\n",
    "\n",
    "# handle special case with spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"gimme that\")\n",
    "print(f\"Not handle : {[t.text for t in doc]}\")\n",
    "\n",
    "# Add special case rule\n",
    "special_case = [{ORTH: \"gim\"}, {ORTH: \"me\"}]\n",
    "nlp.tokenizer.add_special_case(\"gimme\", special_case)\n",
    "doc = nlp(\"gimme that again\")\n",
    "print(f\"Special case : {[t.text for t in doc]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TOKEN', 'Apple'),\n",
       " ('TOKEN', 'is'),\n",
       " ('TOKEN', 'looking'),\n",
       " ('TOKEN', 'at'),\n",
       " ('TOKEN', 'buying'),\n",
       " ('TOKEN', 'U.K.'),\n",
       " ('TOKEN', 'startup'),\n",
       " ('TOKEN', 'for'),\n",
       " ('PREFIX', '$'),\n",
       " ('TOKEN', '1'),\n",
       " ('TOKEN', 'billion')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Debug\n",
    "nlp.tokenizer.explain(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '-', 'world.', ':)']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "# Define special cases\n",
    "special_cases = {\":)\": [{\"ORTH\": \":)\"}]}\n",
    "\n",
    "# Define custom regexes\n",
    "prefix_re = re.compile(r'''^[\\[\\(\"']''')         # Match [, (, \", or '\n",
    "suffix_re = re.compile(r'''[\\]\\)\"']$''')         # Match ], ), \", or '\n",
    "infix_re = re.compile(r'''[-~]''')               # Match - or ~ as infix\n",
    "simple_url_re = re.compile(r'''^https?://''')    # Match basic http/https\n",
    "\n",
    "# Custom tokenizer factory\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab,\n",
    "                     rules=special_cases,\n",
    "                     prefix_search=prefix_re.search,\n",
    "                     suffix_search=suffix_re.search,\n",
    "                     infix_finditer=infix_re.finditer,\n",
    "                     url_match=simple_url_re.match)\n",
    "\n",
    "# Load language model and assign custom tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "# Test\n",
    "doc = nlp(\"hello-world. :)\")\n",
    "print([t.text for t in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"What's\", 'happened', 'to', 'me?', 'he', 'thought.', 'It', \"wasn't\", 'a', 'dream.']\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Basic whitespace tokenizer \n",
    "from spacy.tokens import Doc\n",
    "\n",
    "class WhitespaceTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(\" \")\n",
    "        spaces = [True] * len(words)\n",
    "        # Avoid zero-length tokens\n",
    "        for i, word in enumerate(words):\n",
    "            if word == \"\":\n",
    "                words[i] = \" \"\n",
    "                spaces[i] = False\n",
    "        # Remove the final trailing space\n",
    "        if words[-1] == \" \":\n",
    "            words = words[0:-1]\n",
    "            spaces = spaces[0:-1]\n",
    "        else:\n",
    "           spaces[-1] = False\n",
    "\n",
    "        return Doc(self.vocab, words=words, spaces=spaces)\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)\n",
    "doc = nlp(\"What's happened to me? he thought. It wasn't a dream.\")\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ikbnyb8rDKv9"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Read through spaCy-101 and if you're interested, check out their course\n",
    "# on spaCy itself (link on the page).\n",
    "# https://spacy.io/usage/spacy-101\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /home/amiche/anaconda3/envs/nlp/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /home/amiche/anaconda3/envs/nlp/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: regex, joblib, nltk\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [nltk][32m2/3\u001b[0m [nltk]b]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.4.2 nltk-3.9.1 regex-2024.11.6\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "MMArLP91DKUW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', \"'s\", 'go', 'to', 'N.Y.C.', 'for', 'the', 'weekend', '.']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# EXERCISE: Look up how to tokenize the sentence below using NLTK. The imports \n",
    "# are done for you. Does the NLTK tokenizer handle \"N.Y.C.\" correctly?\n",
    "#\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "s = \"Let's go to N.Y.C. for the weekend.\"\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(s)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMbm9tTakDdy"
   },
   "source": [
    "**NOTE**: Different tokenizers will give subtly different results based on the rules they use. Experiment with different tokenizers and use the one best suited for your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUsfYCpVT4nI"
   },
   "source": [
    "# Basic Preprocessing\n",
    "## Case-Folding, Stop Word Removal, Stemming, and Lemmatization.\n",
    "\n",
    "Course module for this demo:\n",
    "https://www.nlpdemystified.org/course/basic-preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gaj23tgd7Su"
   },
   "source": [
    "**NOTE: If the notebook timed out, you may need to re-upgrade spaCy and re-install the language model as follows:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mg6dga4JePf2"
   },
   "outputs": [],
   "source": [
    "!pip install -U spacy==3.*\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgDDrCeI8f-4"
   },
   "source": [
    "spaCy performs all these preprocessing steps (except stemming) behind the scenes for you. Inline with its non-destructive policy, the tokens aren't modified directly. Rather, each **Token** object has a number of attributes which can help you get views of your document with these pre-processing steps applied. The attributes a **Token** has can be found here:<br>\n",
    "https://spacy.io/api/token#attributes\n",
    "<br><br>\n",
    "More information about spaCy's processing pipeline:<br>\n",
    "https://spacy.io/usage/processing-pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "jDEMR6En1j3H"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "s = \"He told Dr. Lovato that he was done with the tests and would post the results shortly.\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwA1ct0obYlR"
   },
   "source": [
    "### Case-Folding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biBPWrVd9BrK"
   },
   "source": [
    "View your document with case-folding using the *lower_* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'told', 'Dr.', 'Lovato', 'that', 'he', 'was', 'done', 'with', 'the', 'tests', 'and', 'would', 'post', 'the', 'results', 'shortly', '.']\n"
     ]
    }
   ],
   "source": [
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "1nt4RpzdgQQL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'told', 'dr.', 'lovato', 'that', 'he', 'was', 'done', 'with', 'the', 'tests', 'and', 'would', 'post', 'the', 'results', 'shortly', '.']\n"
     ]
    }
   ],
   "source": [
    "print([t.lower_ for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HL46I4sH9OMq"
   },
   "source": [
    "You can also apply conditions when generating these views. For example, we can skip case-folding if a token is the start of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "IO0PQ8IFhOlZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[He, 'told', 'dr.', 'lovato', 'that', 'he', 'was', 'done', 'with', 'the', 'tests', 'and', 'would', 'post', 'the', 'results', 'shortly', '.']\n"
     ]
    }
   ],
   "source": [
    "print([t.lower_ if not t.is_sent_start else t for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7pTz8XJbmaT"
   },
   "source": [
    "### Stop Word Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZLqqmHa9cRx"
   },
   "source": [
    "spaCy comes with a default stop word list. To view your document with stop words removed, you can use the *is_stop* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "9kvXbuDEhOxu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'something', 'give', 'and', 'were', 'therein', '’ve', 'see', 'how', 'him', 'his', 'until', 'always', 'before', 'such', 'make', 'say', 'anyhow', 'bottom', 'though', 'using', 'everywhere', 'everyone', 'due', 'back', 'than', \"'ve\", 'one', \"'d\", 'had', '‘ll', 'eleven', 'this', 'afterwards', 'did', 'towards', '’re', 'hers', 'what', 'never', 'much', 'am', 'sometime', 'well', 'used', 'she', 'when', 'without', 'on', 'quite', 'that', 'herself', 'n’t', 'thereafter', '’s', 'sixty', 'get', 'serious', 'hereafter', 'whereby', 'formerly', 'someone', 'somewhere', 'nor', 'these', 'none', 'yet', 'amongst', 'meanwhile', 'at', 'anyone', 'other', 'as', 'two', 'please', 'further', 'from', 'forty', 'it', 'the', 'many', 'several', 'you', '’m', 'behind', 'call', 'some', 'alone', 'noone', 'nobody', 'who', 'keep', 'whereafter', 'just', 'both', 'latter', 'elsewhere', 'however', 'nothing', 'sometimes', 'become', 'really', 'now', 'while', 'yourself', 'fifteen', 'hereby', 'over', 'move', 'might', 'amount', 'take', 'along', 'across', \"n't\", 'same', 'cannot', 'there', 'except', 'whenever', 'whoever', 'first', 'regarding', 'during', 'own', 'six', 'will', 'yourselves', 'n‘t', 'i', 'third', 'beside', '‘m', 'your', 'almost', 'beyond', 'per', 'seems', '‘ve', 'thru', 'once', 'seem', 'may', 'us', 'more', 'each', 'neither', 'anywhere', 'latterly', 'above', 'its', 'together', 'about', 'top', 'last', 'no', 'against', 'onto', 'upon', 'indeed', 'became', 'around', 'thereby', 'thence', 'various', 'every', 'for', 'full', 'up', 'whom', 'hence', 'all', 'was', 'others', 'do', 'not', 'between', 'although', 'next', 'rather', 'nevertheless', 'because', 'out', 'only', 'ever', 'otherwise', 'part', 'whither', 'too', 'twelve', 'an', 'through', 'via', 'thereupon', '‘d', \"'m\", 'three', \"'re\", 'besides', 'within', 'by', 'her', 'already', '‘s', 'with', 'whereas', 'have', 'is', 'below', 'nine', 'name', 'go', '‘re', 'else', 'down', 'among', 'becoming', 'our', 'should', 'could', 'show', 'them', 'ours', \"'ll\", 'himself', 'made', 'unless', 're', \"'s\", 'but', 'themselves', 'fifty', 'even', 'must', '’d', 'a', 'whereupon', 'any', 'to', 'me', 'why', 'few', 'be', 'since', 'ourselves', 'herein', 'put', 'five', 'which', 'whatever', 'of', 'namely', 'mostly', 'very', 'their', 'side', 'still', 'beforehand', 'either', 'seeming', 'wherever', 'doing', 'mine', 'whole', 'empty', 'again', 'anything', 'can', 'my', 'being', 'ten', 'whether', 'whose', 'becomes', 'hereupon', 'also', 'thus', 'seemed', 'here', 'where', 'he', 'nowhere', 'ca', 'somehow', 'does', 'perhaps', 'are', 'most', 'done', 'those', 'front', 'they', 'therefore', 'if', 'less', 'often', 'throughout', 'least', 'wherein', 'everything', 'whence', '’ll', 'enough', 'in', 'so', 'eight', 'we', 'moreover', 'former', 'would', 'yours', 'another', 'under', 'anyway', 'itself', 'been', 'four', 'or', 'twenty', 'has', 'off', 'then', 'hundred', 'myself', 'toward', 'into', 'after'}\n",
      "326\n"
     ]
    }
   ],
   "source": [
    "# spaCy's default stop word list.\n",
    "print(nlp.Defaults.stop_words)\n",
    "print(len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "oAS1xmgOhO5y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[told, Dr., Lovato, tests, post, results, shortly, .]\n"
     ]
    }
   ],
   "source": [
    "print([t for t in doc if not t.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPd1aiLrbqcK"
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKidP32Y_qcE"
   },
   "source": [
    "It's similar with lemmatization. You can view your document with lemmatization applied through the *lemma_* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "fhdRleESkzTu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'he'),\n",
       " ('told', 'tell'),\n",
       " ('Dr.', 'Dr.'),\n",
       " ('Lovato', 'Lovato'),\n",
       " ('that', 'that'),\n",
       " ('he', 'he'),\n",
       " ('was', 'be'),\n",
       " ('done', 'do'),\n",
       " ('with', 'with'),\n",
       " ('the', 'the'),\n",
       " ('tests', 'test'),\n",
       " ('and', 'and'),\n",
       " ('would', 'would'),\n",
       " ('post', 'post'),\n",
       " ('the', 'the'),\n",
       " ('results', 'result'),\n",
       " ('shortly', 'shortly'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t.text, t.lemma_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuaQJPjEjADE"
   },
   "source": [
    "### Basic Preprocessing Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌱 What Does a Stemmer Do?\n",
    "\n",
    "A **stemmer** is a tool or algorithm that **reduces a word to its base or root form** — called the **stem** — by stripping off **prefixes** or **suffixes**.\n",
    "\n",
    "However, the resulting **stem**:\n",
    "\n",
    "* **May not be a valid word** in the language (unlike a lemma).\n",
    "* Is **not necessarily the same** as a dictionary root.\n",
    "\n",
    "### 🧪 Examples:\n",
    "\n",
    "| Original Word | Stemmed Form |\n",
    "| ------------- | ------------ |\n",
    "| running       | run          |\n",
    "| studies       | studi        |\n",
    "| easily        | easili       |\n",
    "| happiness     | happi        |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Why Is Stemming Necessary in NLP?\n",
    "\n",
    "In many NLP tasks, we don’t need all the different **forms** of a word — we just care about its **core meaning**.\n",
    "\n",
    "### ✅ Use cases:\n",
    "\n",
    "1. **Search engines / Information retrieval:**\n",
    "\n",
    "   * If someone searches for “**run**”, you also want to match “**running**”, “**ran**”, “**runs**”.\n",
    "   * Stemming helps normalize all of these to a common base (e.g., “run”).\n",
    "\n",
    "2. **Text classification / Topic modeling / Clustering:**\n",
    "\n",
    "   * Reducing word variations improves signal-to-noise ratio.\n",
    "   * Helps reduce the **dimensionality** of the vocabulary space.\n",
    "\n",
    "3. **Faster training:**\n",
    "\n",
    "   * In tasks like spam detection or sentiment analysis, you don’t care if it’s “played”, “playing”, or “plays” — they all get reduced to “play”.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚨 Stemming ≠ Lemmatization\n",
    "\n",
    "| Feature  | **Stemming**                        | **Lemmatization**                  |\n",
    "| -------- | ----------------------------------- | ---------------------------------- |\n",
    "| Result   | Rough root (not always a real word) | Dictionary root (lemma)            |\n",
    "| Speed    | Fast (rule-based)                   | Slower (uses grammar & dictionary) |\n",
    "| Accuracy | Lower                               | Higher                             |\n",
    "| Example  | \"studies\" → \"studi\"                 | \"studies\" → \"study\"                |\n",
    "\n",
    "So, stemming is a **quick-and-dirty** solution, often used when:\n",
    "\n",
    "* You don’t need grammatical correctness\n",
    "* You want speed\n",
    "* You’re handling a **large corpus** or building a **prototype**\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 When Should You Use a Stemmer?\n",
    "\n",
    "✅ Use it when:\n",
    "\n",
    "* You’re doing document matching, text search, keyword analysis\n",
    "* You want fast preprocessing without worrying about correctness\n",
    "* You’re okay with some over- or under-stemming\n",
    "\n",
    "❌ Avoid it when:\n",
    "\n",
    "* You need linguistically accurate output (e.g., chatbots, grammar checkers)\n",
    "* You're working with multilingual, morphologically rich languages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uNdkuJqCA2A"
   },
   "source": [
    "spaCy doesn't support stemming natively. But for completeness, we can stem using **NLTK**. Specifically, we can use the *Snowball stemmer* which is an improved version of the *Porter stemmer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "_HQzMurVB13l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available languages for SnowballStemmer\n",
      "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n",
      "Tokens : ['He', 'told', 'Dr.', 'Lovato', 'that', 'he', 'was', 'done', 'with', 'the', 'tests', 'and', 'would', 'post', 'the', 'results', 'shortly', '.']\n",
      "Stemmer : ['he', 'told', 'dr.', 'lovato', 'that', 'he', 'was', 'done', 'with', 'the', 'test', 'and', 'would', 'post', 'the', 'result', 'short', '.']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# EXERCISE: Find out how to intialize the SnowballStemmer, then tokenize\n",
    "# and stem the sentence below.\n",
    "#\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s = 'He told Dr. Lovato that he was done with the tests and would post the results shortly.'\n",
    "\n",
    "# Initialize the stemmer here.\n",
    "print(\"Available languages for SnowballStemmer\")\n",
    "print(\" \".join(SnowballStemmer.languages))\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Tokenize, stem, and print the tokens.\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer  = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(s)\n",
    "\n",
    "stemmed_tokens  = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "print(f\"Tokens : {tokens}\")\n",
    "print(f\"Stemmer : {stemmed_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "wOXJI061npqN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Stop Words: ['He', 'the', 'everything', 'he', 'about', 'the', 'even', 'though', 'it', 'put', 'his', 'own', 'at']\n",
      "After Adding 'told': ['He', 'told', 'the', 'everything', 'he', 'about', 'the', 'even', 'though', 'it', 'put', 'his', 'own', 'at']\n",
      "After Removing 'told': ['He', 'the', 'everything', 'he', 'about', 'the', 'even', 'though', 'it', 'put', 'his', 'own', 'at']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# EXERCISE: Find out how to add and remove your own stop words in spaCy. Add the \n",
    "# word 'told' as a stop word, test that it works, then remove it from \n",
    "# the stop word list.\n",
    "#\n",
    "s = \"He told the committee everything he knew about the incident, even though it put his own job at risk.\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(s)\n",
    "\n",
    "# Before adding 'told' as stop word\n",
    "print(f\"Original Stop Words: {[token.text for token in doc if token.is_stop]}\")\n",
    "\n",
    "# Add 'told' as a stop word\n",
    "nlp.vocab[\"told\"].is_stop = True\n",
    "doc = nlp(s)\n",
    "print(f\"After Adding 'told': {[token.text for token in doc if token.is_stop]}\")\n",
    "\n",
    "# Remove 'told' from stop words\n",
    "nlp.vocab[\"told\"].is_stop = False\n",
    "doc = nlp(s)\n",
    "print(f\"After Removing 'told': {[token.text for token in doc if token.is_stop]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "RLcCYIy-lP1u"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Read up on how to add your own custom attributes to Token objects\n",
    "# and try adding one of your own.\n",
    "# https://spacy.io/usage/processing-pipelines#custom-components-attributes\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9HLYYUt1kOP"
   },
   "source": [
    "#Advanced Preprocessing\n",
    "\n",
    "## Part-of-Speech Tagging, Named Entity Recognition, and Parsing.\n",
    "\n",
    "Course module for this demo:\n",
    "https://www.nlpdemystified.org/course/advanced-preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfBqaH9feymn"
   },
   "source": [
    "**NOTE: If the notebook timed out, you may need to re-upgrade spaCy and re-install the language model as follows:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xOdySsre_sP"
   },
   "outputs": [],
   "source": [
    "!pip install -U spacy==3.*\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tr5SqjHwSWpI"
   },
   "source": [
    "spaCy performs Part-of-Speech (POS) tagging, Named Entity Recognition (NER), and parsing as part of its default pipeline in the *nlp* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shgWRMCq1kmy"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "s = \"John watched an old movie at the cinema.\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwMQgciGb3or"
   },
   "source": [
    "### Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA9LDzULTW1_"
   },
   "source": [
    "POS tags can be accessed through the *pos_* attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-9YRcSZ1kqq"
   },
   "outputs": [],
   "source": [
    "[(t.text, t.pos_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UZcgwejnYm8"
   },
   "source": [
    "To get a description for a POS tag, we can use _spacy.explain_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9SXNvnmnW5e"
   },
   "outputs": [],
   "source": [
    "spacy.explain('PROPN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_WbFDZ-Tqu9"
   },
   "source": [
    "The POS tags above are called *course-grained* tags. You can also access *fine-grained* tags through the *tag_* attribute. Fine-grained tags provide more detailed information about a token such as its tense and, if a word is a pronoun, what specific type of pronoun it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Z5oDzNr1kt2"
   },
   "outputs": [],
   "source": [
    "[(t.text, t.tag_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPOaN9yOUN-I"
   },
   "source": [
    "So **NNP** refers specifically to a _singular pronoun_, and **VBD** is a verb in *past tense*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnfLDxoG1kxf"
   },
   "outputs": [],
   "source": [
    "print(spacy.explain('NNP'))\n",
    "print(spacy.explain('VBD'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jte6K6HJb750"
   },
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2J2BjPyqWFEf"
   },
   "source": [
    "There are multiple ways to access named entities. One way is through the *ent_type_* attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWjNrX6koNVj"
   },
   "outputs": [],
   "source": [
    "s = \"Volkswagen is developing an electric sedan which could potentially come to America next fall.\"\n",
    "doc = nlp(s)\n",
    "\n",
    "[(t.text, t.ent_type_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJL4wfS6Wp9N"
   },
   "source": [
    "You can view spaCy's named entities annotations here:<br>\n",
    "https://spacy.io/api/annotation#named-entities\n",
    "\n",
    "or use _spacy.explain_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iu4OiPwDo9So"
   },
   "outputs": [],
   "source": [
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7p8IcNGpBTP"
   },
   "source": [
    "You can also check if a token is an entity before printing it by checking whether the _ent_type_ (note the lack of trailing underscore) attribute is non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aBng8zdvjly"
   },
   "outputs": [],
   "source": [
    "print([(t.text, t.ent_type_) for t in doc if t.ent_type != 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lNS65_2XJIY"
   },
   "source": [
    "Another way is through the _ents_ property of the **Doc** object. Here, we iterate through _ents_ and print the entity itself and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSCzxs02vjdL"
   },
   "outputs": [],
   "source": [
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHfZmta8XX9Y"
   },
   "source": [
    "Note how \"next fall\" is outputted above as a single span when you use _ents_.\n",
    "<br><br>\n",
    "You can also access the positions of entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSzwRD0MvjTN"
   },
   "outputs": [],
   "source": [
    "print([(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvvQ9_7FdEHT"
   },
   "source": [
    "spaCy is bundled with visualizers for both parsing and named entities.<br>\n",
    "https://spacy.io/usage/visualizers\n",
    "<br><br>\n",
    "Here, we visualize the entities in our sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87eLywmVZCdw"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# We need to set the 'jupyter' variable to True in order to output\n",
    "# the visualization directly. Otherwise, you'll get raw HTML.\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkNmqelTwTLZ"
   },
   "source": [
    "For domain-specific corpora, an NER tagger may need to be further fine-tuned. Here, we may want _The Martian_ tagged as a \"FILM\" (assuming that's our goal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bcIaah29MME"
   },
   "outputs": [],
   "source": [
    "s = \"Ridley Scott directed The Martian.\"\n",
    "doc = nlp(s)\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noGuG3JvcEfs"
   },
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppWrztdJeO3J"
   },
   "source": [
    "Let's first visualize a parse to make it easier to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrvfA1TEvjJT"
   },
   "outputs": [],
   "source": [
    "s = \"She enrolled in the course at the university.\"\n",
    "doc = nlp(s)\n",
    "\n",
    "# Note the 'style' argument is assigned a 'dep' flag this time around.\n",
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRN7_SQ-fO5H"
   },
   "source": [
    "The visualization above is for a dependency parse (spaCy doesn't come with a constituency parser). For each pair of depencencies, spaCy visualizes the child (pointed to), the head (pointed from), and their relationship (the label arc). You can view the dependency annotations here:<br>\n",
    "https://spacy.io/api/annotation#dependency-parsing\n",
    "\n",
    "You can also use *spacy.explain* to get information on a particular annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvz1bLTZfqmv"
   },
   "outputs": [],
   "source": [
    "spacy.explain('nsubj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCvHyqHggIpd"
   },
   "source": [
    "The dependency labels themselves can be accessed through the *dep_* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iX_BgpMVoNaj"
   },
   "outputs": [],
   "source": [
    "[(t.text, t.dep_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tt7zLq0ugR7O"
   },
   "source": [
    "Note how the word 'enrolled' is the _ROOT_.\n",
    "<br><br>\n",
    "But the labels above don't show how the words are related to each other (the arcs). To get a better idea, you can print the head of each dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X15EOIq0oNfF"
   },
   "outputs": [],
   "source": [
    "[(t.text, t.dep_, t.head.text) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFXPL37Rg2xm"
   },
   "source": [
    "### Using spaCy's Matcher to find patterns\n",
    "spaCy comes with a host of pattern-matching functionality. Beyond regex, spaCy can match on a variety of attributes such as POS tags, entity labels, lemmas, dependencies, entire phrases, and a lot more. You can learn more here:<br>\n",
    "https://spacy.io/usage/rule-based-matching<br>\n",
    "https://explosion.ai/demos/matcher\n",
    "<br><br>\n",
    "Here, we try to search for patterns that may be useful for a hospitality bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6v4hVnYmJuaK"
   },
   "outputs": [],
   "source": [
    "# The general Matcher is one of multiple matcher objects\n",
    "# included with spaCy.\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# We initialize the Matcher with the spaCy vocab object, which contains\n",
    "# words along with their labels and entities.\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "s = \"I want to book a hotel room.\"\n",
    "doc = nlp(s)\n",
    "\n",
    "# Patterns are expressed as an ordered sequence. Here, we're looking\n",
    "# to match occurrences starting with a 'book' string followed by\n",
    "# a determiner (DET) POS tag, then a noun POS tag.\n",
    "# The OP key marks the match as optional in some way.\n",
    "\n",
    "# Here, the DET POS (marked with '?') will match 0 or 1 times, and\n",
    "# the NOUN POS (marked with '+') will match 1 or more times.\n",
    "# See this link for more information:\n",
    "# https://spacy.io/usage/rule-based-matching#quantifiers\n",
    "pattern = [\n",
    "  {'TEXT': 'book'},\n",
    "  {'POS': 'DET', 'OP': '?'},\n",
    "  {'POS': 'NOUN', 'OP': '+'},\n",
    "]\n",
    "\n",
    "# We give our pattern a label and pass it to the matcher.\n",
    "matcher.add('USER_INTENT', [pattern])\n",
    "\n",
    "# Run the matcher over the doc.\n",
    "matches = matcher(doc)\n",
    "\n",
    "# For each match, the matcher returns a tuple specifying a match id, start, \n",
    "# and end of the match.\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dygcIKF9plib"
   },
   "source": [
    "The code above demonstrates the Matcher but is brittle.\n",
    "- What if \"book\" is capitalized?\n",
    "- What if a user types \"reserve\" instead of \"book\"?\n",
    "- How can we match on \"hotel room\" as a compound noun?\n",
    "- What if a user types \"book a flight and hotel room\"?\n",
    "\n",
    "Can you think of how you would handle these cases?\n",
    "<br><br>\n",
    "We could come up more rules to match different patterns, or perhaps just search for keywords based on POS and entities (e.g. a country) and present the user with a bunch of possible intentions and let them choose one, or have a bunch of different interpretation functions submit answers and select the most likely one based on what was historically accepted most often. We can also ask clarifying questions to narrow things down.\n",
    "<br><br>\n",
    "For example, for the last sentence, you could have a function scan through the **Doc** object's *noun_chunks* (phrases that have a noun as their head) and isolate keywords there along with potential conjunctions (e.g. \"and\").<br>\n",
    "https://spacy.io/usage/linguistic-features#noun-chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xctXGD5K5Gvr"
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"I want to book a flight and hotel room in Berlin.\")\n",
    "for noun_phrase in doc.noun_chunks:\n",
    "  print(\"phrase: {}, root head: {}\".format(noun_phrase, noun_phrase.root.head))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMsHWX-9EvXU"
   },
   "source": [
    "Using pure rules is a good place to start or prototype (especially if the domain is narrow with a tight set of use cases) but as our requirements get more sophisticated, we'll need to blend in other approaches such as classical models or perhaps deep learning (at the very least, maybe tune existing neural networks). spaCy's models can be updated with more examples to fine-tune predictions.<br>\n",
    "https://spacy.io/usage/training<br>\n",
    "<br>\n",
    "We'll keep learning more approaches as the course progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knyuUv9cqsoY"
   },
   "source": [
    "### Talkin' like Yoda\n",
    "Languages like English are built around the _subject-verb-object_ pattern. But if you're familiar with Yoda from Star Wars, he famously speaks in an _object-subject-verb pattern_. Using the information in a dependency parse, we can turn basic English sentences into Yoda-speak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9AydbEIqsRQ"
   },
   "outputs": [],
   "source": [
    "def yodize(s: str):\n",
    "  doc = nlp(s)\n",
    "  for t in doc:\n",
    "    if t.dep_ == \"ROOT\":\n",
    "\n",
    "      # Assuming our sentence is of the form subject-verb-object, we take \n",
    "      # everything after the root (likely verb) and put it in front, and \n",
    "      # likewise take everything before the root, and put it after.\n",
    "      seq = [doc[t.i + 1: -1].text, doc[0: t.i].text, t.text + '.']\n",
    "      seq[0] = seq[0].capitalize()\n",
    "      print(' '.join(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uIa8Cziwqqnf"
   },
   "outputs": [],
   "source": [
    "yodize(\"I will fly to Texas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofEnieaJZ8eX"
   },
   "source": [
    "This is ok for simple sentences but starts getting weird with longer, more convoluted sentences. What are some ways you would improve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V92TUxWioNtq"
   },
   "source": [
    "### Advanced Preprocessing Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ltil7XSyzMe"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Learn how to extend spaCy's NER models. Specifically, how to add new\n",
    "# entity names and entity types. \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1P58pxYkoN0j"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: using doc.ents, identify and print the dates in this sentence.\n",
    "# Expected output: ['Feb 13th', 'Feb 24th']\n",
    "#\n",
    "s = \"We'll be in Osaka on Feb 13th and leave on Feb 24th.\"\n",
    "doc = nlp(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVFi0bxCoN4N"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Read about spaCy's PhraseMatcher\n",
    "# https://spacy.io/usage/rule-based-matching#phrasematcher\n",
    "#\n",
    "# Using the PhraseMatcher, find the start and end index of all occurrences \n",
    "# of 'Caesar Augustus' and 'Roman Empire' (case-insensitive).\n",
    "#\n",
    "# Expected output: [(0, 2), (15, 17)]\n",
    "#\n",
    "from spacy.matcher import PhraseMatcher\n",
    "s = \"Caesar Augustus was the founder of the Roman Principate (the first phase of the Roman Empire).\"\n",
    "doc = nlp(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nhN7p9G8taJ"
   },
   "source": [
    "# Additional Reading and Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bM4G2KWa8wXO"
   },
   "source": [
    "Read through this page to learn more about spaCy's language processing pipeline including what's going on under the hood, how to create custom components, disable certain components (e.g. NER) when they're unneeded, optimization tips, and best practices:<br>\n",
    "https://spacy.io/usage/processing-pipelines\n",
    "<br><br>\n",
    "Take the free and succinct spaCy course (available in multiple languages):<br>\n",
    "https://course.spacy.io/\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "nlpdemystified-preprocessing.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
